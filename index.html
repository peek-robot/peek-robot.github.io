<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta name="description"
content="PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies">
<meta name="keywords" content="Robot Learning, Imitation Learning, VLAs, Zero-Shot Generalization, Robot Manipulation">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies</title>

<link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
rel="stylesheet">

<link rel="stylesheet" href="./static/css/bulma.min.css">
<link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
<link rel="stylesheet" href="./static/css/bulma-slider.min.css">
<link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
<link rel="stylesheet"
href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
<link rel="stylesheet" href="./static/css/index.css">
<link rel="icon" href="./static/images/favicon.png">

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script defer src="./static/js/fontawesome.all.min.js"></script>
<script src="./static/js/bulma-carousel.min.js"></script>
<script src="./static/js/bulma-slider.min.js"></script>
<script src="./static/js/index.js"></script>
  <!-- Include MathJax -->
<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
  <script type="text/javascript">
    // Ensure MathJax is fully loaded before configuring it
    window.onload = function() {
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [['$', '$']]  // This ensures inline math works with $
        }
      });

      // Optionally, reprocess math formulas on page load
      MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
    };
  </script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
            <h1 class="title is-1 publication-title" style="font-size:2.8rem;">
            <img src="./static/images/peek_emoji.png" alt="ReWiND Logo" style="height:1em; vertical-align:middle; margin-right:0.2em;">
            PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies</h1>
	  <!--<h3 class="title is-4 conference-authors"><a target="_blank" href="https://iclr.cc/">ICLR 2025</a></h3>-->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://jessezhang.net/">Jesse Zhang</a><sup>*1</sup>,
            </span>
            <span class="author-block">
              <a href="https://memmelma.github.io/">Marius Memmel</a><sup>*1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://minjunkevink.github.io/">Kevin Kim</a><sup>3</sup>,
            </span>
            <br>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a><sup>1,4</sup>,
            </span>
            <span class="author-block">
              <a href="https://jessethomason.com/">Jesse Thomason</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://fabioramos.github.io/Home.html">Fabio Ramos</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://ebiyik.github.io/">Erdem Bıyık</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~abhgupta/">Abhishek Gupta</a><sup>†1</sup>,
            </span>
            <span class="author-block">
              <a href="https://anqili.github.io/">Anqi Li</a><sup>†2</sup>,
            </span>
          </div>


          <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>University of Washington,</span>
              <span class="author-block"><sup>2</sup>NVIDIA,</span>
              <span class="author-block"><sup>3</sup>University of Southern California,</span>
              <span class="author-block"><sup>4</sup>Allen Institute for AI</span>
          </div>
          <br> 
          <div class="is-size-7 publication-authors">
              <span class="author-block"><sup>*</sup>Co-first Author</span>
              <span class="author-block"><sup>†</sup>Equal Advising</span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- ArXiv -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2502.05485" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Demo Link. -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/liyi14/HAMSTER_beta"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Data Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
		      <i class="fa-solid fa-database"></i>
                  </span>
                  <span>Data (Coming Soon)</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>


  <div class="container is-max-desktop">

    <section class="section">
      <div class="container is-max-desktop">
        <div class="is-centered has-text-centered">
          <video id="teaser_video" width=100% muted autoplay controls style="border-radius:10px;" margin="auto">
            <source src="figs/hamster_teaser_v2.mp4">
          </video>
        </div>
      </div>
    </section>




    <!--/ Paper video. -->
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Summary</h2>
	<p>
	    <figure>
	    <img src="./static/images/peek_teaser.jpg" style="width: 75%; height: auto;">
	    </figure>
        <div class="content has-text-justified">
          <br>
	  <b>PEEK</b> enhances the zero-shot generalization ability of any RGB-input manipulation policy by showing policies
    <i>where to focus on</i> and <i>what to do</i>. This guidance is 
    given to the policy via a VLM that predicts <i>paths</i> and <i>masking points</i> to 
    draw onto the policy's input images in closed-loop.
    This allows policies to focus on just <i>how</i> to execute low-level actions.
	  <br>


	  <!--<b>Motivation:</b> Large models have shown strong open-world generalization to complex problems in vision and language, but they have been relatively more difficult to deploy in robotics. This challenge stems from several factors, the foremost of which is the lack of scalable robotic training data since this requires expensive on-robot collection.
	  <br>
	  <b>Highlights:</b> We study a class of hierarchical VLA models, where high-level VLMs are trained on relatively cheap data to produce semantically meaningful intermediate predictions such as 2D paths indicating desired behavior. These predicted 2D paths can serve as guidance for low-level control policies that are 3D-aware and capable of precise manipulation. We show that separating prediction into semantic high-level predictions, and 3D-aware low-level predictions allows such hierarchical VLA policies to transfer across significant domain gaps, for instance from simulation to the real world or across scenes with widely varying visual appearance. Doing so allows for the usage of cheap, abundant data sources beyond teleoperated on-robot data thereby enabling broad semantic and visual generalization.
         </p>
        </div>
      </div>
    </div>
  -->
    <!--/ Abstract. -->
  </div>
  <div class="content has-text-justified">
    <h4 class="title is-5">1. VLM Fine-tuning and Data Preparation</h4>
    <b>Overview:</b>
    <p>
      To help generalization of the policy, the VLM itself needs to be able to generalize well to new tasks.
      We fine-tune a pre-trained VLM on a large, automatically labeled robotics dataset to produce <i>paths</i> and <i>masking points</i>.
      Paths help the policy understand <i>what</i> to do, and masking points help the policy understand <i>where</i> to focus on.
    </p>
    <b>Data Labeling Pipeline:</b>
    <figure>
    <img src="./static/images/peek_data_labeling.jpg">
    </figure>
    <p>
    The data labeling pipeline: (1) Detect task-relevant moving points, (2) mask irrelevant areas and extract gripper paths, (3) segment trajectories, and (4) generate gripper paths and masking points for each segment.
    This scalable labeling pipeline allows to label 20 robotics datasets from Open-X!
    </p> 
    <b>VLM Fine-tuning:</b>
    <p>
      We fine-tune a VILA-1.5-3b on the automatically labeled data to produce <i>paths</i> and <i>masking points</i> conditioned on robot image observations and task instructions.
    </p>
  <h4 class="title is-5">2. Policy Training and Inference</h4>
    <figure>
    <img src="./static/images/peek_policy_training.jpg"></a>
    </figure>
    <p> At training time, we use the VLM to predict paths and masks every $H$ steps in closed loop.
      These paths and masks are drawn onto all images that the policy sees, $o_t^{p,m}$, where it predicts low-level environment actions.
      At inference time, the VLM is used in the same way, constantly predicting updated paths and masks in closed loop during policy execution.
      This process is compatible with any RGB-input policy! 
    </p>
  <hr>
  </div>

</section>

<section class="section">

  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <h2 class="title is-2">Experiments and Results</h2>
    </div>
    <div class="content has-text-centered">
      <img src="static/images/peek_results.jpg">
      <br>
      <b>
        PEEK improves performance of 2D (ACT, $\pi_0$) and 3D imitation learning (3DDA) policies in generalization to new task variations.
      We separate results into a variety of sections demonstrating various capabilities of HAMSTER below:
      </b>
    </div>
    <hr>
  </div>
  <br>

  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <h3 class="title is-3">Semantic Generalization</h3>
    </div>

  <div class="columns is-centered">
  <div class="column">
    <div class="content">
      <video id="dollyzoom" autoplay controls muted loop playsinline height="100%" onloadstart="this.playbackRate=1">
        <source src="figs/semantic_generalization_1.mp4" type="video/mp4" class="figure-img img-fluid rounded">
      </video>
      <p><b>Task: Put the M on Mona Lisa; Put the E on Einstein</b></p>
    </div>
  </div>
  <div class="column">
    <div class="content">
      <video id="dollyzoom" autoplay controls muted loop playsinline height="100%" onloadstart="this.playbackRate=1">
        <source src="figs/semantic_generalization_2.mp4" type="video/mp4" class="figure-img img-fluid rounded">
      </video>
      <p><b>Task: Put the coke can on Jensen Huang</b></p>
    </div>
  </div>
  <div class="column">
    <div class="content">
      <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%" onloadstart="this.playbackRate=1">
        <source src="figs/semantic_generalization_3.mp4" type="video/mp4" class="figure-img img-fluid rounded">
      </video>
      <p><b>Task: Put the S block on the plate pointed to by the arrow</b></p>
    </div>
  </div>
  </div>

  <!--<div class="columns is-centered">
    <div class="column">
      <div class="content">
        <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%" onloadstart="this.playbackRate=2">
          <source src="figs/ep_2_pick_up_the_energy_bar_and_put_it_in_the_red_bowl.MP4" type="video/mp4" class="figure-img img-fluid rounded">
        </video>
      </div>
    </div>
    <div class="column">
      <div class="content">
        <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%" onloadstart="this.playbackRate=2">
          <source src="figs/ep_3_pick_up_the_smile_face_and_put_it_in_the_red_bowl.MP4" type="video/mp4" class="figure-img img-fluid rounded">
        </video>
      </div>
    </div>
  </div>-->
    <hr>
  <br>
    </div>

  <!--<div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <h3 class="title is-3">Robustness to Clutter</h3>
    </div>
    <div class="columns is-centered">
    
      <div class="column">
        <div class="content">
          <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%" onloadstart="this.playbackRate=2">
            <source src="figs/ep_6_pick_up_the_green_pepper_and_put_it_in_the_red_bowl__1_.MP4" type="video/mp4" class="figure-img img-fluid rounded">
          </video>
        </div>
      </div>

      <div class="column">
        <div class="content">
          <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%" onloadstart="this.playbackRate=2">
            <source src="figs/ep_1_pick_up_the_garlic_and_put_it_in_the_pan.MP4" type="video/mp4" class="figure-img img-fluid rounded">
          </video>
        </div>
      </div>

    </div>
    <hr>
  <br>
  </div> -->

  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <h3 class="title is-3">Long Horizon, Uninterrupted Task Execution</h3>
    </div>

  <div class="columns is-centered">
  <div class="column">
    <div class="content">
      <video id="dollyzoom" autoplay controls muted loop playsinline height="100%" onloadstart="this.playbackRate=1">
        <source src="figs/long_horizon_1_small.mp4" type="video/mp4" class="figure-img img-fluid rounded">
      </video>
    </div>
  </div>
  <div class="column">
    <div class="content">
      <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%" onloadstart="this.playbackRate=1">
        <source src="figs/long_horizon_2_small.mp4" type="video/mp4" class="figure-img img-fluid rounded">
      </video>
    </div>
  </div>
  </div>

  <hr>
  </div>
  <br>
  
  <!--<div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <h3 class="title is-3">VLM Visual Generalization</h3>
    <br>
    </div>
    <hr>
  </div> -->

  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <h3 class="title is-3">Off Domain Data VLM Fine-tuning helps with Path Generation</h3>
    <br>
    </div>
  </div>
    <div class="container is-max-desktop is-centered">
      <h6 class="title is-6">RLBench Simulation Data helps with path generation of real-world tasks similar to those in the off-domain dataset.</h6>
      <img src="figs/simulation_helps_hamster.png">
      </div>
      <br>
      <br>
    <div class="container is-max-desktop is-centered">
      <h5 class="title is-6">Comparison to RT-Trajectory which uses a pre-trained VLM to generate paths w/o fine-tuning. Fine-tuning on off-domain data to generate paths performs better! </h5>
        <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
          <thead>
            <tr>
              <th>Method</th>
              <th>VLM</th>
              <th>Finetuning Data</th>
              <th>Average Human Rank (lower is better)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>RT-Traj. (Gu et al. 2023)</td>
              <td>0-shot GPT-4o</td>
              <td>-</td>
              <td>3.47</td>
            </tr>
            <tr>
              <td>RT-Traj. (Gu et al. 2023)</td>
              <td>Code as Policies GPT-4o (Liang et al. 2022)</td>
              <td>-</td>
              <td>3.41</td>
            </tr>
            <tr>
              <td>HAMSTER (ours)</td>
              <td>VILA (Lin et al. 2023)</td>
              <td>Our off-domain data $ \tilde{\mathcal{D}}_\text{off} $ without simulated RLBench</td>
              <td>2.13</td>
            </tr>
            <tr>
              <td>HAMSTER (ours)</td>
              <td>VILA (Lin et al. 2023)</td>
              <td>Our off-domain data $ \tilde{\mathcal{D}}_\text{off} $</td>
              <td><b>1.40</b></td>
            </tr>
          </tbody>
        </table>
        
        <p>
          Human evaluation shows HAMSTER, with simulation data, excels in capturing spatial and semantic info across tasks, outperforming zero-shot VLM-based trajectory generation (Gu et al., 2023).
        </p>
    <br>
  <!--<h5 class="title is-6">Fine-tuning the VLM on paths automatically generated from human hand data.</h5>
  <div class="columns is-centered">
  <div class="column">
    <div class="content">
      <video id="dollyzoom" autoplay controls muted loop playsinline height="100%" onloadstart="this.playbackRate=1">
        <source src="figs/hand_tracking_example.mp4" type="video/mp4" class="figure-img img-fluid rounded">
      </video>
    </div>
  </div>
  <p><b>TODO:</b> fill in with an example image of video of block stacking</p>
  <div class="column">
    <div class="content">
      <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%" onloadstart="this.playbackRate=2">
        <source src="figs/TODO" type="video/mp4" class="figure-img img-fluid rounded">
      </video>
    </div>
  </div>
</div> -->

  <hr>
    </div>

  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <h3 class="title is-3">3D Aware Tasks</h3>
    </div>
    <!--<h5 class="title is-6">HAMSTER's VLM can reason about 3D spatial relationships (top, middle, bottom) just from RGB input:</h5>
    <img src="figs/3d_reasoning_image.jpg">-->

  <h5 class="title is-6">The 3D low-level policy can reason about various heights depending on the objects, <i>despite being conditioned on identical paths!</i> All 3 videos below are conditioned on the same path despite the objects being different</h5>
  <div class="columns is-centered">
  <div class="column">
    <div class="content">
      <video id="dollyzoom" autoplay controls muted loop playsinline height="100%" onloadstart="this.playbackRate=5">
        <source src="figs/3d_vid_1.mp4" type="video/mp4" class="figure-img img-fluid rounded">
      </video>
      <p>The low-level policy successfully puts the grapes into the white bowl.</p>
    </div>
  </div>
  <div class="column">
    <div class="content">
      <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%" onloadstart="this.playbackRate=5">
        <source src="figs/3d_vid_2.mp4" type="video/mp4" class="figure-img img-fluid rounded">
      </video>
      <p>The low-level 3D policy reasons about the different height of the milk carton to put it into the white bowl.</p>
    </div>
  </div>
  <div class="column">
    <div class="content">
      <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%" onloadstart="this.playbackRate=5">
        <source src="figs/3d_vid_3.mp4" type="video/mp4" class="figure-img img-fluid rounded">
      </video>
      <p>The 3D policy reasons about the different height of the red mug to successfully put the grapes into it.</p>
    </div>
  </div>
</div>
  <hr>
  </div>
  <br>
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <h3 class="title is-3">Dexterous and Non-Prehensile</h3>
    </div>

  <h5 class="title is-6">HAMSTER can perform dexterous and non-prehensile tasks.</i></h5>
  <div class="columns is-centered">
  <div class="column">
    <div class="content">
      <video id="dollyzoom" autoplay controls muted loop playsinline height="100%" onloadstart="this.playbackRate=1">
        <source src="figs/dexterous_1_small.mp4" type="video/mp4" class="figure-img img-fluid rounded">
      </video>
    </div>
  </div>
  <div class="column">
    <div class="content">
      <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%" onloadstart="this.playbackRate=1">
        <source src="figs/dexterous_2.mp4" type="video/mp4" class="figure-img img-fluid rounded">
      </video>
    </div>
  </div>
  </div>
  <div class="columns is-centered">
    <div class="column">
      <div class="content">
        <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%" onloadstart="this.playbackRate=1">
          <source src="figs/dexterous_3.mp4" type="video/mp4" class="figure-img img-fluid rounded">
        </video>
      </div>
    </div>
    <div class="column">
      <div class="content">
        <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%" onloadstart="this.playbackRate=1">
          <source src="figs/dexterous_4.mp4" type="video/mp4" class="figure-img img-fluid rounded">
        </video>
      </div>
    </div>
  </div>
  <hr>
  </div>
  <br>
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <h3 class="title is-3">Retaining Base VLM Capabilities</h3>
    </div>

    <h5 class="title is-6">Despite fine-tuning HAMSTER still retains base VLM capabilities. For example, HAMSTER's VLM can generalize to new views:</h5>

    <div class="columns is-centered has-text-centered">
      <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" width="70%" onloadstart="this.playbackRate=1">
        <source src="figs/new_view_generalization_small.mp4" type="video/mp4" class="figure-img img-fluid rounded">
      </video>
    </div>
  <hr>
  </div>
  <br>
  <!--<div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <h3 class="title is-3">Inference Speed</h3>
    </div>
  <hr>
  </div>
  <br>-->
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <h3 class="title is-3">Comparison against OpenVLA Pre-Trained with Our Sim Data</h3>
    </div>
    <h5 class="title is-6">Unlike HAMSTER, OpenVLA does not improve when pre-trained with RLBench simulation data. Hierarchy helps enable cross-domain learning in VLAs!</h5>
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%" onloadstart="this.playbackRate=1">
            <source src="figs/hamster_vs_openvla_sim_data_ours.mp4" type="video/mp4" class="figure-img img-fluid rounded">
          </video>
          <b>HAMSTER</b>
        </div>
      </div>
      <div class="column">
        <div class="content has-text-centered">
          <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%" onloadstart="this.playbackRate=1">
            <source src="figs/hamster_vs_openvla_sim_data_openvla.mp4" type="video/mp4" class="figure-img img-fluid rounded">
          </video>
          <b>OpenVLA w/ RLBench Pre-Training</b>
        </div>
      </div>
    <hr>
  </div>
  <hr>
  </div>
  <br>

  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <h3 class="title is-3">HAMSTER is Robust to Different Prompts</h3>
    </div>

    <h5 class="title is-6">Below we demonstrate the HAMSTER's VLM path generation is robust to various input prompts. The first row contains the original prompt HAMSTER was trained with,
      and second row contains new prompts with changes bolded.
    </h5>

    <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
      <thead>
        <tr>
          <th>Prompt</th>
          <th>Drawn Path</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td width="70%; vertical-align; top"><b>Original Prompt</b>: In the image, please execute the command described in &lt;quest&gt;move the coke to Taylor Swift&lt;/quest&gt;.  
            Provide a sequence of points denoting the trajectory of a robot gripper to achieve the goal.  
            Format your answer as a list of tuples enclosed by &lt;ans&gt; and &lt;/ans&gt; tags. For example:  
            &lt;ans&gt;[(0.25, 0.32), (0.32, 0.17), (0.13, 0.24), &lt;action&gt;Open Gripper&lt;/action&gt;, (0.74, 0.21), &lt;action&gt;Close Gripper&lt;/action&gt;, ...]&lt;/ans&gt;  
            The tuple denotes point x and y location of the end effector of the gripper in the image. The action tags indicate the gripper action.  
            The coordinates should be floats ranging between 0 and 1, indicating the relative locations of the points in the image.
            </td>
          <td style="width: 30%; text-align: center;">
            <img src="figs/prompt_robustness_1.png" style="max-width: 100%; height: auto;" alt="Prompt Image">
          </td>
        </tr>
        <!-- <tr>
          <td>In the image, please execute the command described in &lt;quest&gt;<b>help me to have the coke on the Taylor Swift</b>&lt;/quest&gt;.  
            Provide a sequence of points denoting the trajectory of a robot gripper to achieve the goal.  
            Format your answer as a list of tuples enclosed by &lt;ans&gt; and &lt;/ans&gt; tags. For example:  
            &lt;ans&gt;[(0.25, 0.32), (0.32, 0.17), (0.13, 0.24), &lt;action&gt;Open Gripper&lt;/action&gt;, (0.74, 0.21), &lt;action&gt;Close Gripper&lt;/action&gt;, ...]&lt;/ans&gt;  
            The tuple denotes point x and y location of the end effector of the gripper in the image. The action tags indicate the gripper action.  
            The coordinates should be floats ranging between 0 and 1, indicating the relative locations of the points in the image.  
            </td>
          <td style="width: 30%; text-align: center;">
            <img src="figs/prompt_robustness_2.png" style="max-width: 100%; height: auto;" alt="Prompt Image">
          </td>
        </tr>
        <tr>
          <td>
            In the <b>provided</b> image, <b>perform the task</b> described in &lt;quest&gt;<b>pick up the coke and place it on the photo of Taylor Swift</b>&lt;/quest&gt;. <b>Generate</b> a sequence of points <b>representing</b> the trajectory of a robot gripper to <b>accomplish</b> the objective.  
            <b>Present the output</b> as a list of tuples encapsulated within &lt;ans&gt; and &lt;/ans&gt; tags. <b>For instance</b>:  
            &lt;ans&gt;[(0.25, 0.32), (0.32, 0.17), (0.13, 0.24), &lt;action&gt;Open Gripper&lt;/action&gt;, (0.74, 0.21), &lt;action&gt;Close Gripper&lt;/action&gt;, ...]&lt;/ans&gt;  
            <b>Each</b> tuple <b>should</b> denote the x and y coordinates of the gripper’s end-effector as floating-point values ranging from 0 to 1, <b>representing</b> relative positions in the image. <b>Actions like opening or closing the gripper should be indicated within &lt;action&gt; tags.</b>
          </td>
          <td style="width: 30%; text-align: center;">
            <img src="figs/prompt_robustness_3.png" style="max-width: 100%; height: auto;" alt="Prompt Image">
          </td>
        </tr>
        <tr>
          <td>
            In the <b>provided</b> image, <b>perform the task</b> described in &lt;quest&gt;<b>I want the coke on Taylor Swift</b>&lt;/quest&gt;. <b>Generate</b> a sequence of points <b>representing</b> the trajectory of a robot gripper to <b>accomplish</b> the objective.  
            <b>Present the output</b> as a list of tuples encapsulated within &lt;ans&gt; and &lt;/ans&gt; tags. <b>For instance:  
            &lt;ans&gt;[(0.74, 0.21), &lt;action&gt;Close Gripper&lt;/action&gt;, (0.25, 0.32), (0.32, 0.17), (0.13, 0.24), &lt;action&gt;Open Gripper&lt;/action&gt;, ...]</b>&lt;/ans&gt;  
            <b>Each</b> tuple <b>should</b> denote the x and y coordinates of the gripper’s end-effector as floating-point values ranging from 0 to 1, <b>representing</b> relative positions in the image. <b>Actions like opening or closing the gripper should be indicated within &lt;action&gt; tags.</b>
          </td>
          <td style="width: 30%; text-align: center;">
            <img src="figs/prompt_robustness_4.png" style="max-width: 100%; height: auto;" alt="Prompt Image">
          </td>
        </tr> -->
        <tr>
          <td>
            In the <b>provided</b> image, <b>perform the task</b> described in &lt;quest&gt;<b>have the coke on the lady</b>&lt;/quest&gt;. <b>Generate</b> a sequence of points <b>representing</b> the trajectory of a robot gripper to <b>accomplish</b> the objective.  
            <b>Present the output</b> as a list of tuples encapsulated within &lt;ans&gt; and &lt;/ans&gt; tags. <b>For instance:  
            &lt;ans&gt;[(0.74, 0.21), &lt;action&gt;Close Gripper&lt;/action&gt;, (0.25, 0.32), (0.32, 0.17), (0.13, 0.24), &lt;action&gt;Open Gripper&lt;/action&gt;, ...]</b>&lt;/ans&gt;  
          </td>
          <td style="width: 50%; text-align: center;">
            <img src="figs/prompt_robustness_5.png" style="max-width: 100%; height: auto;" alt="Prompt Image">
          </td>
        </tr>

      </tbody>
    </table>
    


  <hr>
  </div>
  <br>
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <h3 class="title is-3">HAMSTER Works with Different Low-Level Policies</h3>
    </div>
        <table class="table is-bordered is-narrow is-hoverable is-fullwidth">
          <thead>
            <tr>
              <th>Policy</th>
              <th>Averaged Success Rate (New Tasks)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>RVT2 (Goyal et al. 2024)</td>
              <td>22.11</td>
            </tr>
            <tr>
              <td>HAMSTER + RVT2 </td>
              <td><b>66.56</b></td>
            </tr>
            <tr style="border-bottom:3px solid black">
            </tr>
            <tr>
              <td>3D-DA (Ke et al. 2024)</td>
              <td>16.70</td>
            </tr>
            <tr>
              <td>HAMSTER + 3D-DA</td>
              <td><b>72.55</b></td>
            </tr>
          </tbody>
        </table>
        
        <p>
          HAMSTER performs well with different low-level policies (RVT2, 3D-DA), and demonstrates a significant performance improvement compared to the base policy with both.
         </p>

  <hr>
  </div>
  <br>
<!--<div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <h3 class="title is-3">Additional Experiment Videos</h3>
    </div>
<div class="columns is-centered">
  <div class="column">
    <div class="content">
      <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%" onloadstart="this.playbackRate=2">
        <source src="figs/ep_0_have_the_r_in_the_left_cup.MP4" type="video/mp4" class="figure-img img-fluid rounded">
      </video>
    </div>
  </div>
  <div class="column">
    <div class="content">
      <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%" onloadstart="this.playbackRate=2">
        <source src="figs/ep_0_pick_up_the_left_object_and_put_it_in_the_left_bowl.MP4" type="video/mp4" class="figure-img img-fluid rounded">
      </video>
    </div>
  </div>
</div>
<div class="columns is-centered">
  <div class="column">
    <div class="content">
      <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%" onloadstart="this.playbackRate=2">
        <source src="figs/ep_0_pick_up_the_r_and_put_it_in_the_red_bowl.MP4" type="video/mp4" class="figure-img img-fluid rounded">
      </video>
    </div>
  </div>
  <div class="column">
    <div class="content">
      <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%" onloadstart="this.playbackRate=2">
        <source src="figs/ep_0_push_down_the_drink.MP4" type="video/mp4" class="figure-img img-fluid rounded">
      </video>
    </div>
  </div>
</div>
<div class="columns is-centered">
  <div class="column">
    <div class="content">
      <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%" onloadstart="this.playbackRate=2">
        <source src="figs/ep_1_pick_up_the_middle_object_and_put_it_in_the_left_bowl.MP4" type="video/mp4" class="figure-img img-fluid rounded">
      </video>
    </div>
  </div>

  <div class="column">
    <div class="content">
      <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%" onloadstart="this.playbackRate=2">
        <source src="figs/ep_5_pick_up_the_right_object_and_put_it_in_the_right_bowl.MP4" type="video/mp4" class="figure-img img-fluid rounded">
      </video>
    </div>
  </div>
</div>
<div class="columns is-centered">
  <div class="column">
    <div class="content">
      <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%" onloadstart="this.playbackRate=2">
        <source src="figs/ep_1_push_down_the_object_on_the_left.MP4" type="video/mp4" class="figure-img img-fluid rounded">
      </video>
    </div>
  </div>
  <div class="column">
    <div class="content">
      <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%" onloadstart="this.playbackRate=2">
        <source src="figs/ep_1_the_goal_is_to_have_the_e_in_the_right_cup.MP4" type="video/mp4" class="figure-img img-fluid rounded">
      </video>
    </div>
  </div>
</div>
<div class="columns is-centered">
  <div class="column">
    <div class="content">
      <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%" onloadstart="this.playbackRate=2">
        <source src="figs/ep_3_press_the_right_red_button.MP4" type="video/mp4" class="figure-img img-fluid rounded">
      </video>
    </div>
  </div>
  <div class="column">
    <div class="content">
      <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%" onloadstart="this.playbackRate=2">
        <source src="figs/ep_4_i_want_the_yellow_button_being_pressed.MP4" type="video/mp4" class="figure-img img-fluid rounded">
      </video>
    </div>
  </div>
</div>
<div class="columns is-centered">
  <div class="column">
    <div class="content">
      <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%" onloadstart="this.playbackRate=2">
        <source src="figs/ep_4_pick_up_the_middle_object_and_put_it_in_the_right_bowl.MP4" type="video/mp4" class="figure-img img-fluid rounded">
      </video>
    </div>
  </div>
  <div class="column">
    <div class="content">
      <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%" onloadstart="this.playbackRate=2">
        <source src="figs/ep_4_pick_up_the_mouse_and_put_it_in_the_white_bowl.MP4" type="video/mp4" class="figure-img img-fluid rounded">
      </video>
    </div>
  </div>
</div>
  </div>-->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@inproceedings{zhang2025peek,
      title={PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies}, 
      author={Jesse Zhang and Marius Memmel and Kevin Kim and Dieter Fox and Jesse Thomason and Fabio Ramos and Erdem Bıyık and Abhishek Gupta and Anqi Li},
      booktitle={arXiv:TODO FILL},
      year={2025},
}
  </code></pre>
    </div>
  </section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Based on the awesome open source template <a
              href="https://github.com/nerfies/nerfies.github.io">from here.</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>